{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T13:58:04.683518Z",
     "start_time": "2018-04-29T13:58:04.277006Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T13:58:22.074476Z",
     "start_time": "2018-04-29T13:58:05.169584Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as spio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import absolute as nabs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load, clean, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:12.366547Z",
     "start_time": "2018-04-29T14:00:12.322053Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def impmat(fname = 'M_processed.mat', writ = True):\n",
    "    ''' import matlab crap, and turn it to pickles (or return panda df)'''\n",
    "    mat = spio.loadmat(fname, squeeze_me=True)\n",
    "    M = mat['M'] \n",
    "    head = ['time','ndc1','ndc2','ndc3',\n",
    "            'Trade_Partner_Name', 'Distribution_Center_State','NDC','Distribution_Center_ID_(IC)',\n",
    "    'Distribution_Center_Zip','Eff_Inv_(EU)','Eff_Inv_(PU)','Qty_Ord_(EU)',\n",
    "            'Qty_Ord_(PU)']\n",
    "    # get rid of ndc 1,2,3 because they're pieces of NCD\n",
    "    # also get rid of purchase units, just use eatable\n",
    "    # also get rid of states and zip code\n",
    "    needed = [0,4,6,7,9,11]\n",
    "    head_adj = [head[i] for i in needed] + [\"year\", \"week\"]\n",
    "    data = pd.DataFrame(M, columns=head)\n",
    "    data[\"time\"] = pd.to_datetime(data[\"time\"], format='%Y%m%d', errors='coerce')\n",
    "    data[\"year\"] = data[\"time\"].dt.year\n",
    "    data[\"week\"] = data[\"time\"].dt.week\n",
    "    #data.drop(\"time\", axis=1)\n",
    "    if writ: # h5 allows your variable to be external\n",
    "        dt = pd.HDFStore(\"drugdata.h5\") # don't need to import/export! warning, though: huge\n",
    "        dt['dat'] = data[head_adj] #\n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T19:15:46.721280Z",
     "start_time": "2018-04-16T19:15:46.693245Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_hd5(p = 0, q = 0):\n",
    "    \"\"\"test data and run answers to intro quiz\n",
    "    p is to print head of dataframe\n",
    "    q prints quiz answers\n",
    "    doesn't return anything\n",
    "    mostly for access examples\"\"\"\n",
    "    dt = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "\n",
    "    header = dt.columns.tolist()\n",
    "    # thanks @brock\n",
    "    def q1(df):\n",
    "        return(df.Trade_Partner_Name.unique())\n",
    "    \n",
    "    def q2(df):\n",
    "        q2 = df.groupby('Trade_Partner_Name')['Distribution_Center_ID_(IC)'].nunique()\n",
    "        q2max = q2.max()\n",
    "        return(q2[q2 == q2max])\n",
    "    \n",
    "    def q3(df):\n",
    "        q3df = df.loc[df[\"time\"].dt.year == 2011] # can also use dt.month\n",
    "        q3TotalSales = q3df.groupby('NDC')['Qty_Ord_(EU)'].sum()\n",
    "        #print(q3TotalSales)\n",
    "        q3sorted = q3TotalSales.sort_values(ascending = False).head()\n",
    "        return(q3sorted)\n",
    "    \n",
    "    def q4(df):\n",
    "        q4 = df['NDC'].value_counts()\n",
    "        NDCLessThan60 = q4[q4 < 60]\n",
    "        if (NDCLessThan60.size == 0):\n",
    "            return(None)\n",
    "        else:\n",
    "            return(NDCLessThan60.size)\n",
    "        \n",
    "    def q5(df):\n",
    "        q5 =  df.groupby('NDC')['Qty_Ord_(EU)'].std()\n",
    "        q5max = q5.max()\n",
    "        NDCHighestVariance = q5[q5 == q5max]\n",
    "        return(NDCHighestVariance)\n",
    "    \n",
    "    def q6(df):\n",
    "        q6 = df.groupby('NDC')['Qty_Ord_(EU)'].nunique()\n",
    "        q6ZeroDemand = q6[q6 == 0]\n",
    "        if (q6ZeroDemand.size == 0):\n",
    "            return(None)\n",
    "        else:\n",
    "            return(q6ZeroDemand.size)\n",
    "    \n",
    "    if p:\n",
    "        for col in header:\n",
    "            print(dt[col].head())\n",
    "    if q:\n",
    "        answers = [q1(dt), q2(dt), q3(dt), q4(dt), q5(dt), q6(dt)]\n",
    "        for i, ans in enumerate(answers):\n",
    "            try:\n",
    "                print('Question %d'%(i+1),  ans)\n",
    "            except:\n",
    "                print('Question %d'%(i+1) + str(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2018-04-16T19:16:34.021422Z",
     "start_time": "2018-04-16T19:16:34.019419Z"
=======
     "end_time": "2018-04-29T14:00:29.847793Z",
     "start_time": "2018-04-29T14:00:29.843693Z"
>>>>>>> 4da69f30d9d21b3354f4199120f930795459c157
    }
   },
   "outputs": [],
   "source": [
    "#impmat(); # uncomment if never built h5 file\n",
    "#test_hd5(q=1) # add p=1 or q = 1 to print stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:27.471727Z",
     "start_time": "2018-04-29T14:00:27.453337Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rem_neg_vals():\n",
    "    ''' if you've just imported from the mat file,\n",
    "    you need to run this to change the neg vals to 0 '''\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    # set negative values to 0\n",
    "    df.loc[df['Eff_Inv_(EU)'] < 0,'Eff_Inv_(EU)'] = 0\n",
    "    df.loc[df['Qty_Ord_(EU)'] < 0,'Qty_Ord_(EU)'] = 0\n",
    "    df.loc[df['Eff_Inv_(EU)'].isnull(), 'Eff_Inv_(EU)'] = 0\n",
    "    df.loc[df['Qty_Ord_(EU)'].isnull(), 'Qty_Ord_(EU)'] = 0\n",
    "    #print(df.head())\n",
    "    return(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:31.967667Z",
     "start_time": "2018-04-29T14:00:31.311306Z"
    }
   },
   "outputs": [],
   "source": [
    "rem_neg_vals();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Early queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:33.564877Z",
     "start_time": "2018-04-29T14:00:33.545367Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def weeks():\n",
    "    ''' gives us a list of the weeks as a datetime Series '''\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "\n",
    "    return(pd.to_datetime(df.time.unique()).sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T19:20:15.777685Z",
     "start_time": "2018-04-16T19:20:15.652528Z"
    }
   },
   "outputs": [],
   "source": [
    "weeks();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:39.891487Z",
     "start_time": "2018-04-29T14:00:39.846595Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def sales_exist():\n",
    "    ''' want to check that every week has sales\n",
    "        returns list of drug ids that have data for every year '''\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    useless = {}\n",
    "    years = [i for i in range(2007, 2018)]\n",
    "    for drug in df.NDC.unique():\n",
    "        useless[drug] = []\n",
    "    for year in years:\n",
    "        sales = df.loc[df.time.dt.year == year].groupby('NDC')['Qty_Ord_(EU)'].sum()\n",
    "        for drug in df.NDC.unique():\n",
    "            try:\n",
    "                if sales[drug] == 0:\n",
    "                    #print(drug, year) # have 0 sum\n",
    "                    useless[drug].append(year)\n",
    "            except:\n",
    "                #print(\"broke by\", drug, \"in\", year)\n",
    "                useless[drug].append(year) # have NA or something?\n",
    "    not_useless = []\n",
    "    for did in useless.keys():\n",
    "        if not useless[did]:\n",
    "            not_useless.append(did)\n",
    "    return(not_useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T21:39:40.865848Z",
     "start_time": "2018-04-16T21:39:37.396510Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_exist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:42.080654Z",
     "start_time": "2018-04-29T14:00:42.064138Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_selling(thr, p = 0):\n",
    "    ''' in: minimum contributing percentage threshold\n",
    "        if p, prints number and % of drugs above thr\n",
    "        out: IDs of drugs above thr'''\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    ind_total = df.groupby('NDC')['Qty_Ord_(EU)'].sum()\n",
    "    sortsales = ind_total.sort_values(ascending = False)\n",
    "    #print(sortsales)\n",
    "    total = sum(ind_total.values)\n",
    "    perc_total = 100 * sortsales / total\n",
    "    clipped_above_total = perc_total[perc_total > thr]\n",
    "    if p:\n",
    "        print(len(clipped_above_total), sum(clipped_above_total.values))\n",
    "    enough = sales_exist()\n",
    "    final = [i for i in enough if i in clipped_above_total.axes[0]]\n",
    "    #return(clipped_above_total.axes)\n",
    "    return(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T21:50:30.985715Z",
     "start_time": "2018-04-16T21:50:27.237028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.0, 7.0, 8.0, 32.0, 55.0, 125.0, 141.0, 145.0, 149.0, 154.0]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_selling(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_drugs():\n",
    "    dl = top_selling(1.5)\n",
    "    for drug in dl:\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:45.274482Z",
     "start_time": "2018-04-29T14:00:45.261620Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def helper():\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    \n",
    "    print(df.columns.values)\n",
    "\n",
    "    print(df[\"week\"])\n",
    "#helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - The meaty bits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:49.799408Z",
     "start_time": "2018-04-29T14:00:49.783764Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def sales(Year):\n",
    "    \"\"\"in: range of dates want studied \n",
    "    want to return the list of sales per date\n",
    "    todo: break up by location if we want\"\"\"\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    \n",
    "    sel_drugs = [i for i in top_selling(1.5)[0]] # list of drug ids\n",
    "    dates = df.loc[df.year == Year] # choose only given year\n",
    "    # gives DF of drugs by week; can change to \n",
    "    # ['NDC', 'time', DISTRO_id] if we want later\n",
    "    window = dates.groupby(['NDC', 'year', 'week'])['Qty_Ord_(EU)'].sum()\n",
    "    filt_window = window.loc[sel_drugs] # only want top drugs\n",
    "    return(filt_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2018-04-16T21:50:56.488602Z",
     "start_time": "2018-04-16T21:50:52.485597Z"
=======
     "end_time": "2018-04-11T18:50:01.439165Z",
     "start_time": "2018-04-11T18:50:00.697994Z"
>>>>>>> 4da69f30d9d21b3354f4199120f930795459c157
    }
   },
   "outputs": [],
   "source": [
    "sales(2008);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:52.398214Z",
     "start_time": "2018-04-29T14:00:52.389317Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smape(f, d):\n",
    "    ''' symmetric mean absolute percentage error\n",
    "    in: vectors f = y_hat, d = y \n",
    "    out: the smape, yo '''\n",
    "    n = len(f)\n",
    "    num = np.sum(nabs(f - d))\n",
    "    denom = np.sum(nabs(f) + nabs(d))\n",
    "    return((1/n) * num/denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:53.322242Z",
     "start_time": "2018-04-29T14:00:53.309332Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frame_gen(year):\n",
    "    ''' in: the year starting the frame\n",
    "        out: 2 dfs, 3 years for training and 4th for test'''\n",
    "    window = []\n",
    "    for i in range(4): #3? switched to match AR model\n",
    "        window.append(sales(year + i))\n",
    "    # make a table of 3 years\n",
    "    window = pd.concat(window)\n",
    "    #test_frame = sales(year + 3)\n",
    "    return(window)#, test_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T19:23:53.141462Z",
     "start_time": "2018-04-16T19:23:53.132451Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svr_try_0(tst, trn):\n",
    "    ''' input: testing and training data \n",
    "        out: support vector regression score(?) '''\n",
    "    \n",
    "    from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "    \n",
    "    \n",
    "  #  mapper = DataFrameMapper([('week', le)\n",
    "                             \n",
    "   #                          ])\n",
    "    le = pp.LabelEncoder()\n",
    "    #lb = pp.LabelBinerizer()\n",
    "    y = le.fit_transform(y)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "    print(len(x_train))\n",
    "    \n",
    "    with open(\"out.txt\", \"w\") as f:\n",
    "        for i in range(-2, 2):\n",
    "            svm = SVR(kernel=\"poly\", C=10**i, random_state=0)\n",
    "            svm.fit(x_train, y_train)\n",
    "            f.write(\"c = \" + str(10**i) + \", accuracy = \" + \n",
    "            str(round(svm.score(x_test, y_test), 3)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T15:20:58.269292Z",
     "start_time": "2018-04-29T15:20:57.796957Z"
    }
   },
   "outputs": [],
   "source": [
    "def ARIMA(tst, trn):\n",
    "    ''' Alright, Jack, do your thing. \n",
    "    I don't wanna touch this one. '''\n",
    "   # trivial change for jack\n",
    "    #data = impmat()\n",
    "    #print(data)\n",
    "    df = dt = pd.HDFStore(\"drugdata.h5\")['dat']\n",
    "    df['year'], df['month'] = df['time'].dt.year, df['time'].dt.month\n",
    "    #print(df['time'])\n",
    "    #NDC = 4 has the highest total sales\n",
    "    ndc4 = df.loc[df[\"NDC\"]==6]\n",
    "    print(\"NDC4: \")\n",
    "    print(ndc4)\n",
    "    ndc4TotalSales = ndc4.groupby('time')['Qty_Ord_(EU)'].sum()\n",
    "    print(\"NDC4 Total Sales: \")\n",
    "    print(ndc4TotalSales)\n",
    "    ndc4Times = ndc4.time.unique()\n",
    "    print(\"NDC4 Times: \")\n",
    "    print(ndc4Times)\n",
    "    ndc4Times = np.array(ndc4Times)\n",
    "    ndc4Times.sort()\n",
    "    print(\"NDC4 Times (sorted): \")\n",
    "    print(ndc4Times)\n",
    "    ndc4TotalSales = np.array(ndc4TotalSales)\n",
    "    plt.plot(ndc4Times, ndc4TotalSales, 'bo')\n",
    "    plt.show()\n",
    "    #X = [[ndc4Times], [ndc4TotalSales]]\n",
    "    #X = np.empty([530,2], dtype='datetime64')\n",
    "    autocorrelation_plot(ndc4TotalSales)\n",
    "    plt.show()\n",
    "    #print(X)\n",
    "    #d = {'times': ndc4Times, 'sales': ndc4TotalSales}\n",
    "    df2 = pd.DataFrame(index=ndc4Times, data=ndc4TotalSales, columns=['sales'])\n",
    "    print(df2)\n",
    "    \n",
    "    df2.plot()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    dftest = pd.DataFrame(index=ndc4Times[400:], data=ndc4TotalSales[400:], columns=['sales'])\n",
    "    print(dftest)\n",
    "    \n",
    "    \n",
    "    ########ARIMA#############\n",
    "    \n",
    "    if p:\n",
    "        start = time.time()\n",
    "\n",
    "        autocorrelation_plot(dftest)\n",
    "        plt.show()\n",
    "\n",
    "        model = ARIMA(dftest, order=(10,1,0))\n",
    "        model_fit = model.fit(disp=0)\n",
    "        print(model_fit.summary())\n",
    "        # plot residual errors\n",
    "        residuals = pd.DataFrame(model_fit.resid)\n",
    "        residuals.plot()\n",
    "        plt.show()\n",
    "        residuals.plot(kind='kde')\n",
    "        plt.show()\n",
    "        print(residuals.describe())\n",
    "        \n",
    "        X = dftest.values\n",
    "        size = int(len(X) * 0.66)\n",
    "        train, test = X[0:size], X[size:len(X)]\n",
    "        print(\"Train/test split\")\n",
    "        history = [x for x in train]\n",
    "        predictions = list()\n",
    "        print(\"About to FOR loop\")\n",
    "        for t in range(len(test)):\n",
    "            model = ARIMA(history, order=(10,1,0))\n",
    "            try: model_fit = model.fit(disp=0)\n",
    "            except (ValueError, LinAlgError): pass\n",
    "            output = model_fit.forecast()\n",
    "            yhat = output[0]\n",
    "            predictions.append(yhat)\n",
    "            obs = test[t]\n",
    "            history.append(obs)\n",
    "            print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "        print(\"Out of FOR loop\")\n",
    "        error = mean_squared_error(test, predictions)\n",
    "        print('Test MSE: %.3f' % error)\n",
    "        # plot\n",
    "        plt.plot(test)\n",
    "        plt.plot(predictions, color='red')\n",
    "        plt.show()\n",
    "    \n",
    "        end = time.time()\n",
    "        print(\"This took: \", end-start, \" seconds\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T19:20:42.309859Z",
     "start_time": "2018-04-16T19:20:42.307856Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_stuff(tst, trn):\n",
    "    ''' All you, Collin '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T19:20:42.766430Z",
     "start_time": "2018-04-16T19:20:42.763426Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lasso_stuff(tst, trn):\n",
    "    X = ndc4Times\n",
    "    Y1 = ndc4TotalSales\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    a=10\n",
    "    #print(a)\n",
    "    Y_past = [ Y1 ]\n",
    "    for i in range(a):\n",
    "        Yi = Y_past[len(Y_past)-1]\n",
    "        Yi = np.insert(Yi, 0, 0)\n",
    "        Y_past.append(Yi[0:len(Y1)])\n",
    "    Y_past = np.matrix(Y_past)\n",
    "    #Y_past = np.transpose(Y_past)\n",
    "    #print(Y1)\n",
    "    #print(\"Y: \")\n",
    "    #print(Y_past)\n",
    "    \n",
    "    #print(\"Y shape: \")\n",
    "    #print(Y_past.shape)\n",
    "    \n",
    "    Y_past = np.delete(Y_past, 0, 0)\n",
    "    #print(\"Y: \")\n",
    "    #print(Y_past)\n",
    "    Y = Y1\n",
    "    #print(\"New shape of Y_past: \")\n",
    "    Y_past = np.transpose(Y_past)\n",
    "    #print(Y_past.shape)\n",
    "    \n",
    "\n",
    "    #print(X[0])\n",
    "    Xts = (X - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n",
    "    #print(Xts[0])\n",
    "    Xts = Xts.reshape(Xts.shape[0], 1)\n",
    "    #print(Xts.shape)\n",
    "    \n",
    "    IN = np.hstack((Xts, Y_past))\n",
    "    #print(\"Input matrix: \")\n",
    "    #print(IN)\n",
    "    print(\"Input Shape: \")\n",
    "    print(IN.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    size = int(IN.shape[0]*0.66)\n",
    "    #print(size)\n",
    "    #X_train, X_test = Xts[0:size], Xts[size: len(Xts)]\n",
    "    X_train, X_test = IN[0:size,:], IN[size:IN.shape[0],:]\n",
    "    #[X_train, X_test] = np.vsplit(IN, size) ################\n",
    "    Y_train, Y_test = Y[0:size], Y[size: len(Y)]\n",
    "    X_train_64, X_test_64 = X[0:size], X[size: len(X)]\n",
    "    Y_train_64, Y_test_64 = Y[0:size], Y[size: len(Y)]\n",
    "    \n",
    "    \n",
    "    #X_train = X_train.reshape(-1, 1)\n",
    "    Y_train = Y_train.reshape(-1, 1)\n",
    "    Y_test = Y_test.reshape(-1, 1)\n",
    "    \n",
    "    if p:\n",
    "        print(\"X_train shape: \", X_train.shape)\n",
    "        print(\"X_test shape: \", X_test.shape)\n",
    "        print(\"Y_train shape: \", Y_train.shape)\n",
    "        print(\"Y_test shape: \", Y_test.shape)\n",
    "        print(\"X_train type: \", X_train.dtype)\n",
    "        print(\"X_test type: \", X_test.dtype)\n",
    "        print(\"Y_train type: \", Y_train.dtype)\n",
    "        print(\"Y_test type: \", Y_test.dtype)\n",
    "\n",
    "    regr = LassoCV()\n",
    "    regr.fit(X_train, Y_train)\n",
    "    \n",
    "    pred_trained = []\n",
    "    pred = []\n",
    "    for x1 in X_train:\n",
    "        #if j==0:\n",
    "         #   x1 = x1.reshape(-1,1)\n",
    "        yHat_trained = regr.predict(x1)\n",
    "        pred_trained.append(yHat_trained)\n",
    "    for x in X_test:\n",
    "        #if j==0:\n",
    "         #   x = x.reshape(-1,1)\n",
    "        yHat = regr.predict(x)\n",
    "        pred.append(yHat)\n",
    "        \n",
    "    #X_convert = X_train.reshape(X_train.shape[0]) \n",
    "    X_convert = datetime.utcfromtimestamp(X_train[0,0])\n",
    "    #X_test_64 = datetime.utcfromtimestamp(X_test)\n",
    "    #Y_test_64 = datetime.utcfromtimestamp(Y_test)\n",
    "    #pred_64 = datetime.utcfromtimestamp(pred)\n",
    "    X_convert = np.datetime64(X_convert)\n",
    "    #X_test_64 = np.datetime64(X_test)\n",
    "    #Y_test_64 = np.datetime64(Y_test)\n",
    "    #pred_64 = np.datetime64(pred)\n",
    "    \n",
    "    #print(X_convert)\n",
    "    \n",
    "    plt.plot(X_train_64, Y_train, color='green')\n",
    "    plt.plot(X_train_64, pred_trained, color='red')\n",
    "    plt.plot(X_test_64, pred, color='orange')\n",
    "    plt.plot(X_test_64, Y_test, color='blue')\n",
    "    plt.show()\n",
    "    \n",
    "    error = mean_squared_error(Y_test, pred)\n",
    "    print(\"Error: \", error)\n",
    "    #errors.append(error)\n",
    "    \n",
    "    \n",
    "                    \n",
    "    #plt.plot(J, errors, color='red')\n",
    "    #plt.show()\n",
    "    \n",
    "    end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T22:19:45.455390Z",
     "start_time": "2018-04-16T22:19:45.452387Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def testing_setup():\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    [trn, tst] = frame_gen(2008)\n",
    "    dl = top_selling(1.5)\n",
    "    for drug in dl:\n",
    "        print(drug, trn[drug])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T23:26:12.454475Z",
     "start_time": "2018-04-16T23:26:12.444462Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Auto_regress(drug, X, lag_size, plots=0):\n",
    "    ''' need model for each drug id; maybe pass in main?\n",
    "        test data, train data\n",
    "        lag is number of previous vars\n",
    "        but, model chooses lag size?? '''\n",
    "    # sauce: https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/\n",
    "    #from sklearn.svm import SVR\n",
    "    from statsmodels.tsa.ar_model import AR\n",
    "    \n",
    "    # at this point all drugs have all data for all years, so we can generalize\n",
    "    X_trn = X[drug].values[:157] # first 3 years\n",
    "    X_tst = X[drug].values[157:] # next\n",
    "    if plots:\n",
    "        from statsmodels.graphics.tsaplots import plot_acf\n",
    "        plot_acf(X_trn, lags=lag_size)\n",
    "        plt.show()\n",
    "    model = AR(X_trn)\n",
    "    fits = model.fit()\n",
    "    print(\"Thetas:\", fits.params)\n",
    "    print(\"lag:\", fits.k_ar)\n",
    "    \n",
    "    pred = fits.predict(start=len(X_trn), end=len(X_trn)+len(X_tst)-1, dynamic=True)\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    print(\"MSE:\", mse(X_tst, pred))\n",
    "    plt.plot(X_tst)\n",
    "    plt.plot(pred, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T23:26:12.735827Z",
     "start_time": "2018-04-16T23:26:12.724813Z"
    },
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def main(lag_size):\n",
    "    dt = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    # we are gonna add 3 to this. only want full years too, why not.\n",
    "    spread = [i for i in range(2008, 2013)]\n",
    "    dlist = top_selling(1.5)\n",
    "    \n",
    "    thetaset = np.zeros(shape=(len(dlist), lag_size))# what do we want here???\n",
    "    thetaset = pd.DataFrame(thetaset, index=dlist)\n",
    "    for year in spread: # sliding window for analysis\n",
    "        X = frame_gen(year)\n",
    "        for drug in dlist:\n",
    "            Auto_regress(drug,X,lag_size)\n",
    "        return(0)\n",
    "#         seth = svr_stuff(tst, trn)\n",
    "#         collin = nn_stuff(tst, trn)\n",
    "#         jack = ARIMA(tst, trn)\n",
    "#         maybe = lasso_stuff(tst, trn)\n",
    "#         which = [seth, collin, jack, maybe]\n",
    "#         who = [\"seth\", \"collin\", \"jack\", \"maybe\"]\n",
    "#         best_for_year = who[which.index(min(which))]\n",
    "#         print(\"The best for\", year, \"is\", best_for_year)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-16T23:25:43.738570Z",
     "start_time": "2018-04-16T23:24:45.534796Z"
<<<<<<< HEAD
    }
=======
    },
    "hidden": true
>>>>>>> 4da69f30d9d21b3354f4199120f930795459c157
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thetas: [ -2.53151887e+05   2.66899306e-01   2.87649176e-01   1.09347682e-01\n",
      "   7.83628909e-02   2.56090739e-01  -7.19345227e-02  -1.89046394e-01\n",
      "   2.05497932e-01  -4.30616389e-02   8.47304849e-02  -1.55375569e-01\n",
      "   2.83222764e-02   1.31231028e-01]\n",
      "lag: 13\n",
      "MSE: 3.21873256852e+12\n",
      "Thetas: [  2.37873932e+05   3.20859404e-01   2.69673384e-01   5.76518303e-02\n",
      "   1.40373714e-01   1.34357687e-01  -8.22844634e-02  -8.92731745e-02\n",
      "   6.59051487e-02   4.72977527e-02  -9.38356295e-02   4.40095060e-02\n",
      "  -6.46077536e-02   2.52229486e-02]\n",
      "lag: 13\n",
      "MSE: 151229615977.0\n",
      "Thetas: [ -3.50372707e+04   1.03009834e-01   2.35838907e-02   2.18393791e-01\n",
      "   9.28332923e-02   2.27667419e-01   1.12834534e-01  -6.87576875e-02\n",
      "   1.07012636e-01   3.19997227e-02   6.79399986e-02   2.56175693e-02\n",
      "   2.64057308e-02   1.46675072e-02]\n",
      "lag: 13\n",
      "MSE: 32225542952.5\n",
      "Thetas: [  5.04807334e+04   5.78984894e-02   4.80352043e-03   1.10902242e-01\n",
      "   1.36516665e-01   2.03507154e-01   9.56274071e-02  -2.07606367e-02\n",
      "  -9.14241318e-03   9.81542197e-02  -8.59356048e-02   1.37721972e-01\n",
      "   4.94071912e-02   1.19116838e-01]\n",
      "lag: 13\n",
      "MSE: 16511985192.7\n",
      "Thetas: [  9.05823262e+05   3.91528282e-02  -5.90147490e-03   5.86206891e-04\n",
      "   1.73637160e-01   2.52062204e-01   4.56752882e-02   7.45676147e-02\n",
      "   1.23589419e-01   3.65008792e-02  -5.37275714e-02   7.43407531e-02\n",
      "  -4.91778155e-02   8.15982163e-02]\n",
      "lag: 13\n",
      "MSE: 1.31705803587e+12\n",
      "Thetas: [  4.57660503e+05   2.76006289e-01   1.69367480e-01   5.98973856e-02\n",
      "  -6.35442665e-03   1.22877769e-01  -2.27638977e-01   4.20426690e-02\n",
      "  -3.57809894e-02   9.12636418e-02   6.49234580e-02   5.21278044e-02\n",
      "  -2.76916391e-01   1.25843282e-01]\n",
      "lag: 13\n",
      "MSE: 209544138277.0\n",
      "Thetas: [  1.90918205e+05   3.17513922e-01   3.16684086e-01   7.56830812e-02\n",
      "   1.03327591e-01   1.08056456e-01  -1.54172753e-01  -1.27863716e-01\n",
      "   1.59647381e-01   1.81927420e-02  -2.19393562e-02  -2.21115929e-02\n",
      "   6.79081699e-02  -5.04244058e-02]\n",
      "lag: 13\n",
      "MSE: 111265496750.0\n",
      "Thetas: [  9.85799820e+03   8.66925227e-02   3.68582024e-03  -1.37376145e-01\n",
      "   6.66591432e-02   1.79603458e-01  -4.34812081e-02   1.15891996e-01\n",
      "   1.54274641e-01   1.25810760e-01   9.00590230e-02   1.32220551e-01\n",
      "   6.36163942e-02   9.98421225e-02]\n",
      "lag: 13\n",
      "MSE: 3377148322.65\n",
      "Thetas: [  2.39582832e+05   2.43045330e-01   1.32055972e-01   1.11554470e-01\n",
      "  -4.04552177e-02   2.16999144e-01   3.90849559e-02  -6.40233202e-02\n",
      "  -2.98911063e-02   6.99871046e-02  -3.24397396e-02  -1.70647686e-01\n",
      "  -7.38927228e-02   1.27425231e-01]\n",
      "lag: 13\n",
      "MSE: 31517711080.7\n",
      "Thetas: [  1.02955114e+05  -9.75064596e-03  -9.32753587e-02   1.79271844e-01\n",
      "   1.53639695e-01   2.02121784e-01   7.59750460e-02  -4.19045797e-02\n",
      "  -2.61109603e-02   1.93301891e-01   7.19161579e-02   1.45806665e-01\n",
      "  -4.75530795e-02   1.35592678e-01]\n",
      "lag: 13\n",
      "MSE: 301740933340.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cors: [5, 6, 0, 6, 5, 0, 5, 6, 6, 8]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.1"
=======
   "version": "3.6.2"
>>>>>>> 4da69f30d9d21b3354f4199120f930795459c157
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
